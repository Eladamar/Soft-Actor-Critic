# Soft-Actor-Critic

Simple implementation of soft actor critic algorithm in pytorch.  
Added beta policy and priority replay buffer for improvements.

## Papers

[[1](https://arxiv.org/pdf/1812.05905.pdf)]  Haarnoja, Tuomas, et al. "Soft actor-critic algorithms and applications." arXiv preprint arXiv:1812.05905 (2018).  
[[2](https://arxiv.org/pdf/1511.05952.pdf)] Schaul, Tom, et al. "Prioritized experience replay." arXiv preprint arXiv:1511.05952 (2015).  
[[3](http://proceedings.mlr.press/v70/chou17a/chou17a.pdf)] Chou, Po-Wei, Daniel Maturana, and Sebastian Scherer. "Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution." International conference on machine learning. PMLR, 2017.  

## Other References
Followed [In-depth review of Soft Actor-Critic](https://towardsdatascience.com/in-depth-review-of-soft-actor-critic-91448aba63d4) and [PER](https://github.com/rlcode/per) implementation.

## How to Use
Just open the notebook:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eladamar/Soft-Actor-Critic/blob/main/sac.ipynb)
